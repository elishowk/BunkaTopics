{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea2a84a-2453-4ed7-9a8c-30abf4f1e561",
   "metadata": {},
   "source": [
    "# Generate Topics and launch the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82813768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 17:09:28.290814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-20 17:09:28.290841: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-20 17:09:28.290872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-20 17:09:28.299061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 17:09:29.045921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-20 17:09:38.319676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import webbrowser\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pathlib import Path\n",
    "from bunkatopics.functions.web import launch_web_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f02ee",
   "metadata": {},
   "source": [
    "# Load documents and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff148f13-f1fb-48c7-ad31-cc75195c373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents_path = Path('json_examples/docs.jsonl')\n",
    "#topics_path = Path('json_examples/topics.jsonl')\n",
    "documents_path = Path('json_examples/bourdieu_docs.json')\n",
    "topics_path = Path('json_examples/bourdieu_topics.json')\n",
    "topics = []\n",
    "documents = []\n",
    "# Open the JSONL files and load data\n",
    "# with open(topics_path, 'r') as serialized_topics:\n",
    "#     for line in serialized_topics:\n",
    "#         # Parse each line as a JSON object and append it to the list\n",
    "#         data = json.loads(line)\n",
    "#         topics.append(data)\n",
    "\n",
    "# with open(documents_path, 'r') as serialized_documents:\n",
    "#     for line in serialized_documents:\n",
    "#         # Parse each line as a JSON object and append it to the list\n",
    "#         data = json.loads(line)\n",
    "#         documents.append(data)\n",
    "\n",
    "with open(topics_path, 'r') as serialized_topics:\n",
    "    data = json.load(serialized_topics)\n",
    "    topics = data\n",
    "\n",
    "with open(documents_path, 'r') as serialized_documents:\n",
    "    data = json.load(serialized_documents)\n",
    "    documents = data\n",
    "\n",
    "source_data = {\n",
    "    'documents': documents,\n",
    "    'topics': topics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796b5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elishowk/src/BunkaTopics/web/public/localSearchResults.json\n"
     ]
    }
   ],
   "source": [
    "def transform_and_write(source_data, output_file_path):\n",
    "    \"\"\"\n",
    "    Transform BunkaTopics output to the web front-end format\n",
    "    \"\"\"\n",
    "    left_words = []\n",
    "    right_words = []\n",
    "\n",
    "    for doc in source_data[\"documents\"]:\n",
    "        if doc[\"bourdieu_dimensions\"]:\n",
    "            for dimension in doc[\"bourdieu_dimensions\"]:\n",
    "                left_words += [dimension[\"continuum\"][\"left_words\"]]\n",
    "                right_words += [dimension[\"continuum\"][\"right_words\"]]\n",
    "    # Traduire chaque document en utilisant une list comprehension\n",
    "    dest_data_documents = [{\n",
    "        \"id\": doc[\"doc_id\"],\n",
    "        \"text\": doc[\"content\"],\n",
    "        \"source\": None,  # Pas présent dans les données source\n",
    "        \"language\": \"en\",  # Pas présent dans les données source\n",
    "        \"languages\": [\"en\"],  # Pas présent dans les données source\n",
    "        \"created_at_timestamp_sec\": None,  # Pas présent dans les données source\n",
    "        \"author\": None,  # Pas présent dans les données source\n",
    "        \"embedding_light\": [doc[\"x\"], doc[\"y\"]],\n",
    "        \"topic_ids\": doc[\"term_id\"],\n",
    "        \"rank\": {\n",
    "            \"rank\": doc[\"topic_ranking\"][\"rank\"] if doc[\"topic_ranking\"] else None,\n",
    "            \"rank_per_topic\": {\n",
    "                str(doc[\"topic_id\"]): {\n",
    "                    \"rank\": doc[\"topic_ranking\"][\"rank\"],\n",
    "                    \"score\": None,\n",
    "                    \"score_bin\": None,\n",
    "                    \"count_specific_terms\": None,\n",
    "                    \"specificity_bin\": None,\n",
    "                    \"bunka_score\": None\n",
    "                }\n",
    "            } if doc[\"topic_ranking\"] else {}\n",
    "        },\n",
    "        # doc[\"embedding\"] n'est pas présent dans les données cibles\n",
    "        \"dimensions\": [{\n",
    "            \"id\": \" / \".join([\" \".join(dimension[\"continuum\"][\"left_words\"]), \" \".join(dimension[\"continuum\"][\"right_words\"])]),\n",
    "            \"score\": dimension[\"distance\"]\n",
    "        } for dimension in doc[\"bourdieu_dimensions\"] if doc[\"bourdieu_dimensions\"]],\n",
    "    } for doc in source_data[\"documents\"]]\n",
    "\n",
    "    # Traduire chaque topic en utilisant une list comprehension\n",
    "    dest_data_topics = [{\n",
    "        \"id\": topic[\"topic_id\"],\n",
    "        \"size\": topic[\"size\"],\n",
    "        \"percent\": None,  # Pas présent dans les données source\n",
    "        \"parent_topic_id\": None,  # Pas présent dans les données source\n",
    "        \"centroid\": {\n",
    "            \"cluster_id\": topic[\"topic_id\"],\n",
    "            \"x\": topic[\"x_centroid\"],\n",
    "            \"y\": topic[\"y_centroid\"]\n",
    "        },\n",
    "        \"convex_hull\": {\n",
    "            \"cluster_id\": topic[\"topic_id\"],\n",
    "            \"x_coordinates\": topic[\"convex_hull\"][\"x_coordinates\"],\n",
    "            \"y_coordinates\": topic[\"convex_hull\"][\"y_coordinates\"]\n",
    "        },\n",
    "        \"explanation\": {\n",
    "            \"topic_id\": topic[\"topic_id\"],\n",
    "            \"name\": topic[\"name\"],\n",
    "            \"specific_terms\": topic[\"term_id\"],\n",
    "            \"top_terms\": topic[\"top_term_id\"],\n",
    "            \"top_entities\": topic[\"top_doc_id\"]\n",
    "        }\n",
    "    } for topic in source_data[\"topics\"]]\n",
    "\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        # Écrire l'objet JSON \n",
    "        json.dump({\n",
    "            \"documents\": dest_data_documents,\n",
    "            \"topics\": dest_data_topics,\n",
    "            \"query\": {\n",
    "                \"text\": \"Test\",\n",
    "                \"top_k\": 400,\n",
    "                \"min_doc_retrieved\": 100,\n",
    "                \"max_toxicity\": 0.8,\n",
    "                \"languages\": None,\n",
    "                \"topics\": {\n",
    "                    \"shape\": [\n",
    "                        6,\n",
    "                        2\n",
    "                    ],\n",
    "                    \"convex_hull_interpolation\": True,\n",
    "                    \"min_doc_per_topic\": 20,\n",
    "                    \"ngrams\": [\n",
    "                        1,\n",
    "                        2\n",
    "                    ],\n",
    "                    \"min_count_term\": 3,\n",
    "                    \"top_terms_included\": 20000,\n",
    "                    \"text_type\": \"term_id\",\n",
    "                    \"n_terms_in_name\": 5,\n",
    "                    \"number_top_terms_returned\": 20,\n",
    "                    \"number_specific_terms_returned\": 20,\n",
    "                    \"top_n_specificity_fn\": 200,\n",
    "                    \"specificity_weight\": 6,\n",
    "                    \"popularity_weight\": 3,\n",
    "                    \"feature_binned_number\": 10\n",
    "                },\n",
    "                \"intensity_dimensions\": [\n",
    "                    {\n",
    "                        \"id\": \"arts\",\n",
    "                        \"kind\": \"intensity\",\n",
    "                        \"words\": [\n",
    "                            \"arts\",\n",
    "                            \"sculpture\",\n",
    "                            \"architecture\",\n",
    "                            \"painting\",\n",
    "                            \"drawing\",\n",
    "                            \"music\",\n",
    "                            \"literature\",\n",
    "                            \"poetry\",\n",
    "                            \"theater\",\n",
    "                            \"dance\",\n",
    "                            \"movie\",\n",
    "                            \"photography\",\n",
    "                            \"cinema\",\n",
    "                            \"cooking\",\n",
    "                            \"fashion\"\n",
    "                        ]\n",
    "                    },\n",
    "                ],\n",
    "                \"continuum_dimensions\": [\n",
    "                    {\n",
    "                        \"id\": \"positive / negative\",\n",
    "                        \"kind\": \"continuum\",\n",
    "                        \"left_id\": \"positive\",\n",
    "                        \"right_id\": \"negative\",\n",
    "                        \"left_words\": left_words,\n",
    "                        \"right_words\": right_words\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "            \"nb_documents\": len(source_data[\"documents\"]),\n",
    "        }, outfile, indent=4)\n",
    "\n",
    "\n",
    "output_file_path = Path('/home/elishowk/src/BunkaTopics') / 'web' / 'public' / 'localSearchResults.json'\n",
    "print(output_file_path)\n",
    "\n",
    "transform_and_write(source_data, output_file_path)\n",
    "#launch_web_app(source_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
